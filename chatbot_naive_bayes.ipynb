{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eb874425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages and libraries\n",
    "import nltk\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# naive bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16db0812",
   "metadata": {},
   "source": [
    "### Import and load the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d8642991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intents': [{'tag': 'greetings', 'patterns': ['Hi', 'Hey', 'Hello', 'Hey there', 'Hello restaurant Taiwan'], 'responses': ['Hello and welcome to restaurant Taiwan', 'Hey there', 'Hi, how can I help you?', 'Welcome to restaurant Taiwan, I am your personal assistant. How can I help you?']}, {'tag': 'openinghours', 'patterns': ['what are the opening hours?', 'When is the restaurant closed?', 'Could you please give me the opening hours of the restaurant?', 'When is the restaurant open?'], 'responses': ['The restaurant is open from Thursday-Sunday from 5pm-10:30pm']}, {'tag': 'payments', 'patterns': ['Can i pay with credit card', 'Cash', 'Google Pay', 'Apple Pay', 'AMEX', 'Debit', 'Paypal', 'what are the payment methods?'], 'responses': ['We accept the following payment methods: Cash, VISA, Mastercard']}, {'tag': 'reservation', 'patterns': ['I want to make a reservation', 'Can you please help me to make a reservation?', 'Would you help me to make a reservation?'], 'responses': ['Sure, for how many people would you like to make a reservation?']}, {'tag': 'makereservation', 'patterns': ['I would like to make a reservation for 2 persons', 'I would like to make a reservation for 3 persons', 'i would like to make a reservation for 4 persons', 'I would like to make a reservation for 4 persons', '1', '2', '3', '4', '5', '6'], 'responses': ['The reservation has been made']}, {'tag': 'placeorder', 'patterns': ['I want food', 'I want to order food asap', 'Can you please take my order for food?', 'Take my order please', 'I want to place an order for Chinese food', 'I want to place an order', 'Would you please help me to order food?', 'Can you please order food for me'], 'responses': ['Sure, what would you like to order today?', 'Definitely, what would you like to have today?', \"Certainly, I'll try to help you with that. What are you feeling like eating today?\"]}, {'tag': 'order', 'patterns': ['I want 1 loempia and 2 nasi goreng', '1 loempia and 2 nasi goreng', 'I want 2 tomatensoep and 1 nasi goreng met kip', 'I want', 'I would like to have', 'I would like to order food'], 'responses': ['Okay, your order has been confirmed.']}, {'tag': 'location', 'patterns': ['where is your restaurant?', 'location', 'locate', 'map', 'train', 'car', 'transport', 'What is the address of the restaurant?', 'What is the location of the restaurant?'], 'responses': ['The address of the restaurant is Oude Steenweg 65, 2530 Boechout. Public transport is convenient as it is only a 5min walk.']}, {'tag': 'userthanks', 'patterns': ['well thank you', 'ok thank you', 'Thank you so much', 'Thanks a lot', 'Thank you'], 'responses': [\"I'm glad to be able to help you, see you next time!\", 'Bye! Have a nice day!']}, {'tag': 'goodbye', 'patterns': ['bye', 'see you', 'see ya'], 'responses': ['We hope you are satisfied with our service. Have a good day!', 'We are looking forward to welcome you again in our restaurant']}, {'tag': 'contact', 'patterns': ['How can I contact you?', 'How can I find more information?'], 'responses': ['For more information please contact us by phone 034559544 or email restauranttaiwant@gmail.be']}, {'tag': 'soupitems', 'patterns': ['Could you please provide me the soup dishes?', 'Can you give me the soup dishes', 'Which are the soup dishes?', 'What are the soup dishes?'], 'responses': ['Here is a list of the soup dishes:\\n\\n - Champignons met bamboesoep: 3.9 euro,\\n- Tomatensoep: 3.9 euro,\\n- Kippenleversoep met groenten: 4.5 euro,\\n- Pikante soep: 4.5 euro,\\n- Chinese champignonsoep: 4.5 euro,\\n- Thailandse soep: 5.5 euro,\\n- Aspergesoep met krab: 5.5 euro,\\n- Haaievinnensoep: 5.5 euro,\\n- Kippensoep: 4.5 euro,\\n- Wan tan soep: 5.5 euro']}, {'tag': 'appetizeritems', 'patterns': ['Could you please provide me the appetizer dishes?', 'Can you give me the appetizer dishes', 'Which are the appetizer dishes?', 'What are the appetizers?'], 'responses': ['Here is a list of the appetizer dishes:\\n\\n - loempia met kip: 5.5 euro,\\n- loempia met garnalen: 5.7 euro,\\n- loempia met krab: 6 euro,\\n- kippensate: 7 euro,\\n- chinees voorgerecht: 8 euro,\\n- kroepoek: 3.9 euro,\\n- gebakken varkensribbetjes in lookboter: 8 euro,\\n- gebakken kikkerbilletjes in lookboter: 9.5 euro,\\n- gefrituurde wan tan: 7.5 euro,\\n- dim sum: 9.5 euro']}, {'tag': 'poultryitems', 'patterns': ['Could you please provide me the poultry dishes?', 'Can you give me the poultry dishes', 'Which are the poultry dishes?', 'What are the poultry dishes?'], 'responses': ['Here is a list of the poultry dishes:\\n\\n - kippenfilet met tomatensaus: 14 euro,\\n- kippenfilet met currysaus: 14 euro,\\n- kippenfilet met diverse groenten: 14 euro,\\n- kippensteak met champignons: 14 euro,\\n- kip met ananas en zoetzure saus: 14 euro,\\n- kip op 3 wijzen: 33 euro,\\n- kippenfilet met ananas en groenten: 14 euro,\\n- kippenfilet met chinese champignons: 14.5 euro,\\n- gefrituurde kippenblokjes: 14 euro,\\n- omelet met kip: 14 euro']}, {'tag': 'riceitems', 'patterns': ['Could you please provide me the rice dishes?', 'Can you give me the rice dishes', 'Which are the rice dishes?', 'What are the rice dishes?'], 'responses': ['Here is a list of the rice dishes:\\n\\n - nasi goreng special: 14.5 euro,\\n- nasi goreng met kip: 13 euro,\\n- nasi goreng met varkensvlees: 13 euro,\\n- nasi goreng met chinese garnalen: 17 euro,\\n- nasi goreng met krab: 17 euro,\\n- nasi goreng met kleine garnalen: 14.5 euro']}, {'tag': 'specialitiesitems', 'patterns': ['Could you please provide me the specialities?', 'Can you give me the specialities', 'Which are the specialities?', 'What are the specialities?'], 'responses': [\"Here is a list of the specialities:\\n\\n - rundsvlees op ijzeren plaat: 17 euro,\\n- kippenfilet op ijzeren plaat: 16 euro,\\n- chinese garnalen op ijzeren plaat: 20 euro,\\n- kikkerbilletjes in potje op z'n chinees: 18 euro,\\n- gamba's in curryroomsays: 21 euro,\\n- varkensribbetjes op kantonese wijze: 16 euro,\\n- vlees op z'n sechuan: 15 euro,\\n- chop choy taiwan: 17 euro,\\n- kippenfilet met curry: 15 euro,\\n- nasi goreng taiwan: 17 euro,\\n- tongfilet met ananas en zoetzure saus: 20 euro,\\n- tongfilet in curryroomsaus: 20 euro,\\n- 1/2 eend met ananas en currysaus: 17 euro,\\n- 1/2 eend met ananas en zoetzure saus: 17 euro,\\n- nasi goreng met kip en ananas: 15 euro\"]}, {'tag': 'spicyitems', 'patterns': ['Could you provide me the spicy dishes', 'Can you provide me the spicy dishes?', 'Which are the spicy dishes?', 'Are there any spicy dishes?', 'Which are the spicy dishes?'], 'responses': [\"Here is a list of the spicy dishes:\\n\\n - pikante soep: 4.5 euro,\\n - thailandse soep: 5.5 euro,\\n - kippenfilet met currysaus: 14 euro,\\n - gamba's in curryroomsaus: 21 euro,\\n - vlees op z'n sechuan: 15 euro,\\n - tongfilet in curryroomsaus: 20 euro,\\n - 1/2 eend met ananas en currysaus: 17 euro\"]}, {'tag': 'vegetarianitems', 'patterns': ['Could you provide me the vegetarian dishes', 'Can you provide me the vegetarian dishes?', 'Which are the vegetarian dishes?'], 'responses': ['Most dishes can be made vegetarion, please ask staff for more information.']}, {'tag': 'alcohol', 'patterns': ['alcohol', 'beer', 'Cocktails', 'wine', 'spirits', 'liquor', 'drinks'], 'responses': ['We serve alcoholic beverages from 5pm, to consume our alcoholic beverages you have to be at least 18 years old']}, {'tag': 'delivery', 'patterns': ['ordering', 'home', 'delivery', 'Do you deliver food?'], 'responses': ['Food can not be delivered to your home.']}, {'tag': 'menu', 'patterns': ['menu', 'Could you provide me the menu?', 'Can I see the menu?', 'What is the menu?'], 'responses': ['As our menu is big, you can take a look in our menu on the website! http://restauranttaiwan.be/menu/ ']}, {'tag': 'seats', 'patterns': ['How big is the restaurant?', 'How many seats', 'seats'], 'responses': ['In total we have 62 seats in the restaurant']}]}\n"
     ]
    }
   ],
   "source": [
    "with open('intents.json', 'r') as f:\n",
    "    intents = json.load(f)\n",
    "print(intents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31cdadd",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "10c37858",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = []\n",
    "training_sentences_stem = []\n",
    "training_labels = []\n",
    "labels = []\n",
    "documents = []\n",
    "#responses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "028a9f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Artificial intelligence is a wide topic, and NLP is one of the subsets.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4973efe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    token = nltk.word_tokenize(sentence)\n",
    "    return token\n",
    "\n",
    "#def remove_punctuation_marks(token):\n",
    "#    output = []\n",
    "#    output = [k for k in token if k.isalpha()]\n",
    "#    return output\n",
    "\n",
    "#def remove_punctuation_marks(word):\n",
    "#    output = re.sub(\"[^a-zA-Z']\", \" \", str(word))\n",
    "#    return output\n",
    "\n",
    "def stem(word):\n",
    "    return stemmer.stem(word.lower())\n",
    "\n",
    "def bag_of_words(tokenized_sentence, training_sentences, doc):\n",
    "    \n",
    "    tok_sens = []\n",
    "    bag = []\n",
    "    for w in tokenized_sentence:\n",
    "        word = stem(w)\n",
    "        tok_sens.append(word)\n",
    "        \n",
    "    for w in training_sentences:\n",
    "        bag.append(1) if w in tok_sens else bag.append(0)\n",
    "    #print(bag)\n",
    "    \n",
    "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[labels.index(doc[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "            \n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "56cd216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence = tokenize(sentence)\n",
    "#print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9f88faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence = remove_punctuation_marks(sentence)\n",
    "#print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f5960ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in intents['intents']:\n",
    "    # add to our labels\n",
    "    training_label = intent['tag']\n",
    "    training_labels.append(training_label)\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word\n",
    "        word = tokenize(pattern)\n",
    "        training_sentences.extend(word)\n",
    "        # add document in the corpus\n",
    "        documents.append((word, training_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9adbf533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1966d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for w in training_sentences:\n",
    "#    test = remove_punctuation_marks(w)\n",
    "#    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2acd309f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'hey', 'hello', 'hey', 'there', 'hello', 'restaur', 'taiwan', 'what', 'are', 'the', 'open', 'hour', 'when', 'is', 'the', 'restaur', 'close', 'could', 'you', 'pleas', 'give', 'me', 'the', 'open', 'hour', 'of', 'the', 'restaur', 'when', 'is', 'the', 'restaur', 'open', 'can', 'i', 'pay', 'with', 'credit', 'card', 'cash', 'googl', 'pay', 'appl', 'pay', 'amex', 'debit', 'paypal', 'what', 'are', 'the', 'payment', 'method', 'i', 'want', 'to', 'make', 'a', 'reserv', 'can', 'you', 'pleas', 'help', 'me', 'to', 'make', 'a', 'reserv', 'would', 'you', 'help', 'me', 'to', 'make', 'a', 'reserv', 'i', 'would', 'like', 'to', 'make', 'a', 'reserv', 'for', '2', 'person', 'i', 'would', 'like', 'to', 'make', 'a', 'reserv', 'for', '3', 'person', 'i', 'would', 'like', 'to', 'make', 'a', 'reserv', 'for', '4', 'person', 'i', 'would', 'like', 'to', 'make', 'a', 'reserv', 'for', '4', 'person', '1', '2', '3', '4', '5', '6', 'i', 'want', 'food', 'i', 'want', 'to', 'order', 'food', 'asap', 'can', 'you', 'pleas', 'take', 'my', 'order', 'for', 'food', 'take', 'my', 'order', 'pleas', 'i', 'want', 'to', 'place', 'an', 'order', 'for', 'chines', 'food', 'i', 'want', 'to', 'place', 'an', 'order', 'would', 'you', 'pleas', 'help', 'me', 'to', 'order', 'food', 'can', 'you', 'pleas', 'order', 'food', 'for', 'me', 'i', 'want', '1', 'loempia', 'and', '2', 'nasi', 'goreng', '1', 'loempia', 'and', '2', 'nasi', 'goreng', 'i', 'want', '2', 'tomatensoep', 'and', '1', 'nasi', 'goreng', 'met', 'kip', 'i', 'want', 'i', 'would', 'like', 'to', 'have', 'i', 'would', 'like', 'to', 'order', 'food', 'where', 'is', 'your', 'restaur', 'locat', 'locat', 'map', 'train', 'car', 'transport', 'what', 'is', 'the', 'address', 'of', 'the', 'restaur', 'what', 'is', 'the', 'locat', 'of', 'the', 'restaur', 'well', 'thank', 'you', 'ok', 'thank', 'you', 'thank', 'you', 'so', 'much', 'thank', 'a', 'lot', 'thank', 'you', 'bye', 'see', 'you', 'see', 'ya', 'how', 'can', 'i', 'contact', 'you', 'how', 'can', 'i', 'find', 'more', 'inform', 'could', 'you', 'pleas', 'provid', 'me', 'the', 'soup', 'dish', 'can', 'you', 'give', 'me', 'the', 'soup', 'dish', 'which', 'are', 'the', 'soup', 'dish', 'what', 'are', 'the', 'soup', 'dish', 'could', 'you', 'pleas', 'provid', 'me', 'the', 'appet', 'dish', 'can', 'you', 'give', 'me', 'the', 'appet', 'dish', 'which', 'are', 'the', 'appet', 'dish', 'what', 'are', 'the', 'appet', 'could', 'you', 'pleas', 'provid', 'me', 'the', 'poultri', 'dish', 'can', 'you', 'give', 'me', 'the', 'poultri', 'dish', 'which', 'are', 'the', 'poultri', 'dish', 'what', 'are', 'the', 'poultri', 'dish', 'could', 'you', 'pleas', 'provid', 'me', 'the', 'rice', 'dish', 'can', 'you', 'give', 'me', 'the', 'rice', 'dish', 'which', 'are', 'the', 'rice', 'dish', 'what', 'are', 'the', 'rice', 'dish', 'could', 'you', 'pleas', 'provid', 'me', 'the', 'special', 'can', 'you', 'give', 'me', 'the', 'special', 'which', 'are', 'the', 'special', 'what', 'are', 'the', 'special', 'could', 'you', 'provid', 'me', 'the', 'spici', 'dish', 'can', 'you', 'provid', 'me', 'the', 'spici', 'dish', 'which', 'are', 'the', 'spici', 'dish', 'are', 'there', 'ani', 'spici', 'dish', 'which', 'are', 'the', 'spici', 'dish', 'could', 'you', 'provid', 'me', 'the', 'vegetarian', 'dish', 'can', 'you', 'provid', 'me', 'the', 'vegetarian', 'dish', 'which', 'are', 'the', 'vegetarian', 'dish', 'alcohol', 'beer', 'cocktail', 'wine', 'spirit', 'liquor', 'drink', 'order', 'home', 'deliveri', 'do', 'you', 'deliv', 'food', 'menu', 'could', 'you', 'provid', 'me', 'the', 'menu', 'can', 'i', 'see', 'the', 'menu', 'what', 'is', 'the', 'menu', 'how', 'big', 'is', 'the', 'restaur', 'how', 'mani', 'seat', 'seat']\n"
     ]
    }
   ],
   "source": [
    "ignore_words = ['?', '!', '.', ',']\n",
    "\n",
    "for w in training_sentences:\n",
    "    # remove punctuations\n",
    "    if w not in ignore_words:\n",
    "        # stem the words \n",
    "        word = stem(w)\n",
    "        training_sentences_stem.append(word)\n",
    "\n",
    "print(training_sentences_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8cf8b270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['greetings', 'openinghours', 'payments', 'reservation', 'makereservation', 'placeorder', 'order', 'location', 'userthanks', 'goodbye', 'contact', 'soupitems', 'appetizeritems', 'poultryitems', 'riceitems', 'specialitiesitems', 'spicyitems', 'vegetarianitems', 'alcohol', 'delivery', 'menu', 'seats']\n"
     ]
    }
   ],
   "source": [
    "print(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0163b963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5', '6', 'a', 'address', 'alcohol', 'amex', 'an', 'and', 'ani', 'appet', 'appl', 'are', 'asap', 'beer', 'big', 'bye', 'can', 'car', 'card', 'cash', 'chines', 'close', 'cocktail', 'contact', 'could', 'credit', 'debit', 'deliv', 'deliveri', 'dish', 'do', 'drink', 'find', 'food', 'for', 'give', 'googl', 'goreng', 'have', 'hello', 'help', 'hey', 'hi', 'home', 'hour', 'how', 'i', 'inform', 'is', 'kip', 'like', 'liquor', 'locat', 'loempia', 'lot', 'make', 'mani', 'map', 'me', 'menu', 'met', 'method', 'more', 'much', 'my', 'nasi', 'of', 'ok', 'open', 'order', 'pay', 'payment', 'paypal', 'person', 'place', 'pleas', 'poultri', 'provid', 'reserv', 'restaur', 'rice', 'seat', 'see', 'so', 'soup', 'special', 'spici', 'spirit', 'taiwan', 'take', 'thank', 'the', 'there', 'to', 'tomatensoep', 'train', 'transport', 'vegetarian', 'want', 'well', 'what', 'when', 'where', 'which', 'wine', 'with', 'would', 'ya', 'you', 'your']\n",
      "['alcohol', 'appetizeritems', 'contact', 'delivery', 'goodbye', 'greetings', 'location', 'makereservation', 'menu', 'openinghours', 'order', 'payments', 'placeorder', 'poultryitems', 'reservation', 'riceitems', 'seats', 'soupitems', 'specialitiesitems', 'spicyitems', 'userthanks', 'vegetarianitems']\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates and sort it\n",
    "\n",
    "training_sentences = sorted(set(training_sentences_stem))\n",
    "labels = sorted(set(training_labels))\n",
    "\n",
    "\n",
    "print(training_sentences)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1413efa9",
   "metadata": {},
   "source": [
    "### Create training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bdf67271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "\n",
    "training = []\n",
    "\n",
    "# create an empty array for the output\n",
    "output_empty = [0] * len(labels)\n",
    "print(output_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cd356e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-113-0c09021e30b0>:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  training = np.array(training)\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    pattern_sentence = doc[0]\n",
    "    bag = bag_of_words(pattern_sentence, training_sentences, doc)\n",
    "\n",
    "# shuffle our features and turn into np.array\n",
    "# shuffling randomly and converting into numpy array for faster processing\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# create train and test lists. X - patterns, Y - intents\n",
    "# creating train and test lists\n",
    "X = list(training[:,0])\n",
    "y = list(training[:,1])\n",
    "print(\"Training data created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f1e3edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0273b0f5",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e74a4c",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f59bea6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (76, 22) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-679ca738f1a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mNBclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mNBclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    610\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         \"\"\"\n\u001b[1;32m--> 612\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_class_log_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    824\u001b[0m                         ensure_2d=False, dtype=None)\n\u001b[0;32m    825\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 826\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    827\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'O'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m    862\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 864\u001b[1;33m     raise ValueError(\n\u001b[0m\u001b[0;32m    865\u001b[0m         \u001b[1;34m\"y should be a 1d array, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m         \"got an array of shape {} instead.\".format(shape))\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (76, 22) instead."
     ]
    }
   ],
   "source": [
    "NBclassifier = MultinomialNB(alpha=1)\n",
    "\n",
    "NBclassifier.fit(X_train,y_train)\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171dd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
